# Accelerate configuration for LLaDA training
# This file can be used with: accelerate launch --config_file accelerate_config.yaml train_llada_pretrained.py

compute_environment: LOCAL_MACHINE
distributed_type: MULTI_GPU
downcast_bf16: 'no'
gpu_ids: all
machine_rank: 0
main_process_ip: localhost
main_process_port: 29500
main_training_function: main
mixed_precision: fp16
num_machines: 1
num_processes: 4
rdzv_backend: static
same_network: true
tpu_env: []
tpu_use_cluster: false
tpu_use_sudo: false
use_cpu: false

# NCCL configuration to fix network issues
env:
  NCCL_DEBUG: INFO
  NCCL_IB_DISABLE: 1
  NCCL_P2P_DISABLE: 1
  NCCL_SOCKET_IFNAME: lo
  NCCL_BLOCKING_WAIT: 1
  NCCL_TIMEOUT: 1800
